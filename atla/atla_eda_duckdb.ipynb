{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cace4e9-ba4c-44b6-95cb-de9437e66965",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- there are 60 episodes, so we need a maximum of 6 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314b683-dfe0-4496-9a56-30fcd6c3b952",
   "metadata": {},
   "source": [
    "### 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d10939-c91c-4aad-8ed3-dabf91fbca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa13045-063c-499c-99ab-edd1d3fcfe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVATAR_URL = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-11/avatar.csv'\n",
    "SCENE_URL = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2020/2020-08-11/scene_description.csv'\n",
    "\n",
    "res_avatar = duckdb.sql(f\"select * from '{AVATAR_URL}'\")\n",
    "res_scene = duckdb.sql(f\"select * from '{SCENE_URL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4e6bfb-800a-43ea-be53-358e44488238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'BIGINT', 'YES', None, None, None),\n",
       " ('book', 'VARCHAR', 'YES', None, None, None),\n",
       " ('book_num', 'BIGINT', 'YES', None, None, None),\n",
       " ('chapter', 'VARCHAR', 'YES', None, None, None),\n",
       " ('chapter_num', 'BIGINT', 'YES', None, None, None),\n",
       " ('character', 'VARCHAR', 'YES', None, None, None),\n",
       " ('full_text', 'VARCHAR', 'YES', None, None, None),\n",
       " ('character_words', 'VARCHAR', 'YES', None, None, None),\n",
       " ('writer', 'VARCHAR', 'YES', None, None, None),\n",
       " ('director', 'VARCHAR', 'YES', None, None, None),\n",
       " ('imdb_rating', 'VARCHAR', 'YES', None, None, None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(f\"describe select * from '{AVATAR_URL}'\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ded1cdf-024c-4acb-a838-7690f5650928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'BIGINT', 'YES', None, None, None),\n",
       " ('scene_description', 'VARCHAR', 'YES', None, None, None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(f\"describe select * from '{SCENE_URL}'\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8516a66-b8ae-4bb3-a9be-06d5391a8122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29864cf7-3dda-4ea1-8f44-415bb228584a",
   "metadata": {},
   "source": [
    "### 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4102a1-9ab1-44d4-81d4-7790add1edfd",
   "metadata": {},
   "source": [
    "#### 2.1.1. Words counts per character by episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa7df3d-2838-422d-9dd8-7e81b3989bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_word_counts = duckdb.sql(f\"\"\"\n",
    "select \n",
    "    book_num, chapter_num, character\n",
    "    , sum(len(character_words)) as word_counts\n",
    "from '{AVATAR_URL}'\n",
    "group by \n",
    "    book_num, chapter_num, character\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80054ccf-e45b-4e44-9f5b-8e92cbc58500",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_word_counts_normalized = duckdb.sql(f\"\"\"\n",
    "SELECT \n",
    "    book_num,\n",
    "    chapter_num,\n",
    "    character,\n",
    "    SUM(LENGTH(character_words)) AS char_word_count,\n",
    "    SUM(LENGTH(character_words)) * 100.0 / SUM(SUM(LENGTH(character_words))) OVER (PARTITION BY book_num, chapter_num) AS normalized_word_count, \n",
    "    AVG(TRY_CAST(imdb_rating AS FLOAT)) as imdb_rating,\n",
    "FROM '{AVATAR_URL}'\n",
    "GROUP BY book_num, chapter_num, character\n",
    "HAVING \n",
    "    char_word_count > 100\n",
    "    and count(*) > 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ba5db5-a51e-4679-a890-d4059ae6175a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_num</th>\n",
       "      <th>chapter_num</th>\n",
       "      <th>character</th>\n",
       "      <th>char_word_count</th>\n",
       "      <th>normalized_word_count</th>\n",
       "      <th>imdb_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Zuko</td>\n",
       "      <td>654.0</td>\n",
       "      <td>6.859660</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Mother Superior</td>\n",
       "      <td>406.0</td>\n",
       "      <td>4.258443</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Iroh</td>\n",
       "      <td>322.0</td>\n",
       "      <td>3.377386</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Scene Description</td>\n",
       "      <td>126.0</td>\n",
       "      <td>1.321586</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Aang</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>13.834697</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Bumi</td>\n",
       "      <td>667.0</td>\n",
       "      <td>8.840292</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Zuko</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>21.219351</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Toph</td>\n",
       "      <td>334.0</td>\n",
       "      <td>4.426773</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Sokka</td>\n",
       "      <td>520.0</td>\n",
       "      <td>6.891981</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Aang</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>26.136514</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     book_num  chapter_num          character  char_word_count  \\\n",
       "0           1           15               Zuko            654.0   \n",
       "1           1           15    Mother Superior            406.0   \n",
       "2           1           15               Iroh            322.0   \n",
       "3           1           15  Scene Description            126.0   \n",
       "4           1           15               Aang           1319.0   \n",
       "..        ...          ...                ...              ...   \n",
       "458         3           19               Bumi            667.0   \n",
       "459         3           19               Zuko           1601.0   \n",
       "460         3           19               Toph            334.0   \n",
       "461         3           19              Sokka            520.0   \n",
       "462         3           19               Aang           1972.0   \n",
       "\n",
       "     normalized_word_count  imdb_rating  \n",
       "0                 6.859660          7.9  \n",
       "1                 4.258443          7.9  \n",
       "2                 3.377386          7.9  \n",
       "3                 1.321586          7.9  \n",
       "4                13.834697          7.9  \n",
       "..                     ...          ...  \n",
       "458               8.840292          9.5  \n",
       "459              21.219351          9.5  \n",
       "460               4.426773          9.5  \n",
       "461               6.891981          9.5  \n",
       "462              26.136514          9.5  \n",
       "\n",
       "[463 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_word_counts_normalized.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3c2a3f-f965-4bd7-91f6-45ef9e891389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_word_counts_normalized.df().groupby(['book_num', 'chapter_num']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f13ed4e9-e0f6-44ff-8b11-816d16ff1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_rating_from_normalized_words = duckdb.sql(f\"\"\"\n",
    "WITH normalized_counts as (\n",
    "    SELECT \n",
    "        book_num,\n",
    "        chapter_num,\n",
    "        character,\n",
    "        SUM(LENGTH(character_words)) AS char_word_count,\n",
    "        SUM(LENGTH(character_words)) * 100.0 / SUM(SUM(LENGTH(character_words))) OVER (PARTITION BY book_num, chapter_num) AS normalized_word_count, \n",
    "        AVG(TRY_CAST(imdb_rating AS FLOAT)) as imdb_rating,\n",
    "    FROM '{AVATAR_URL}'\n",
    "    GROUP BY book_num, chapter_num, character\n",
    "    HAVING \n",
    "        char_word_count > 100\n",
    "        and count(*) > 5\n",
    "), character_rating as (\n",
    "    SELECT\n",
    "        character\n",
    "        , SUM(normalized_word_count * imdb_rating) / SUM(normalized_word_count) as avg_rating\n",
    "    from normalized_counts\n",
    "    group by character\n",
    ")\n",
    "\n",
    "select * from character_rating\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac7fee8-0560-4668-af74-1d2e1cac31a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yue</td>\n",
       "      <td>5.963911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Zhao</td>\n",
       "      <td>7.031211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Canyon guide</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Zhang leader</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Gan Jin leader</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Gan Jin tribesman</td>\n",
       "      <td>7.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Tong</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Due</td>\n",
       "      <td>7.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Huu</td>\n",
       "      <td>7.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Tho</td>\n",
       "      <td>7.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Oyaji</td>\n",
       "      <td>7.746066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Mother Superior</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bato</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Calm man</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wu</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meng</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Xu</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Guard</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Pirate captain</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Tyro</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Haru's mother</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Haru</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dock</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Oh</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Storyteller</td>\n",
       "      <td>8.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Teo</td>\n",
       "      <td>8.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mechanist</td>\n",
       "      <td>8.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Old wanderer</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Song</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Senlin Village leader</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Chong</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Dealer</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Earthbender captain</td>\n",
       "      <td>8.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Kanna</td>\n",
       "      <td>8.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Jet</td>\n",
       "      <td>8.307790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Aang</td>\n",
       "      <td>8.380606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Headmaster</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Kwan</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Ticket lady</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>King Bumi</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Ukano</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Shoji</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Than</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Shuzumu</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Yung</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ying</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Bumi</td>\n",
       "      <td>8.412642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Katara</td>\n",
       "      <td>8.443555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Mai</td>\n",
       "      <td>8.473049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Ty Lee</td>\n",
       "      <td>8.478116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 character  avg_rating\n",
       "3                      Yue    5.963911\n",
       "110                   Zhao    7.031211\n",
       "68            Canyon guide    7.100000\n",
       "121           Zhang leader    7.100000\n",
       "93          Gan Jin leader    7.100000\n",
       "41       Gan Jin tribesman    7.100000\n",
       "43                    Tong    7.500000\n",
       "77                     Due    7.700000\n",
       "71                     Huu    7.700000\n",
       "78                     Tho    7.700000\n",
       "84                   Oyaji    7.746066\n",
       "105        Mother Superior    7.900000\n",
       "7                     Bato    7.900000\n",
       "32                Calm man    7.900000\n",
       "14                      Wu    7.900000\n",
       "4                     Meng    7.900000\n",
       "37                      Xu    8.000000\n",
       "99                   Guard    8.000000\n",
       "31          Pirate captain    8.000000\n",
       "52                    Tyro    8.000000\n",
       "40           Haru's mother    8.000000\n",
       "92                    Haru    8.000000\n",
       "8                     Dock    8.000000\n",
       "39                      Oh    8.000000\n",
       "65             Storyteller    8.100000\n",
       "33                     Teo    8.100000\n",
       "45               Mechanist    8.100000\n",
       "87            Old wanderer    8.200000\n",
       "36                    Song    8.200000\n",
       "96   Senlin Village leader    8.200000\n",
       "44                   Chong    8.200000\n",
       "103                 Dealer    8.200000\n",
       "48     Earthbender captain    8.200000\n",
       "119                  Kanna    8.300000\n",
       "62                     Jet    8.307790\n",
       "106                   Aang    8.380606\n",
       "47              Headmaster    8.400000\n",
       "70                    Kwan    8.400000\n",
       "56             Ticket lady    8.400000\n",
       "94               King Bumi    8.400000\n",
       "76                   Ukano    8.400000\n",
       "18                   Shoji    8.400000\n",
       "9                     Than    8.400000\n",
       "115                Shuzumu    8.400000\n",
       "34                    Yung    8.400000\n",
       "10                    Ying    8.400000\n",
       "82                    Bumi    8.412642\n",
       "85                  Katara    8.443555\n",
       "88                     Mai    8.473049\n",
       "118                 Ty Lee    8.478116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_rating_from_normalized_words.df().sort_values(by='avg_rating', ascending=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238e6947-1b8d-423a-b7c3-e5d3ddf122b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_word_counts.df().sort_values(by='word_counts', ascending=True)[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de495ce-11fc-4bdf-bbee-d74e5299ea0a",
   "metadata": {},
   "source": [
    "#### 2.1.2. Character Clustering based on word counts throughout the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7af47bb3-e384-403e-86f7-b865e3c406e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row: character, word count episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61425cf1-3aac-447c-b43b-7fc1aeb663fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_character_word_vector = pd.pivot_table(res_word_counts_normalized.df(), values=['normalized_word_count'], index=['character'], \n",
    "               columns=['book_num', 'chapter_num']).reset_index(drop=False).fillna(0.0) # drop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44532728-dc95-4e7d-bde9-eede2fd984f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th colspan=\"20\" halign=\"left\">normalized_word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_num</th>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chapter_num</th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aang</td>\n",
       "      <td>22.573198</td>\n",
       "      <td>24.486191</td>\n",
       "      <td>23.818840</td>\n",
       "      <td>25.192774</td>\n",
       "      <td>34.644701</td>\n",
       "      <td>7.538701</td>\n",
       "      <td>33.473183</td>\n",
       "      <td>20.917059</td>\n",
       "      <td>17.141792</td>\n",
       "      <td>...</td>\n",
       "      <td>13.824212</td>\n",
       "      <td>29.143308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.008598</td>\n",
       "      <td>7.630322</td>\n",
       "      <td>16.210215</td>\n",
       "      <td>26.136514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.506636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor Sokka</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.799443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actor Zuko</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.153203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actress Aang</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.018305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actress Azula</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.317549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Yung</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Zei</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Zhang leader</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Zhao</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.477942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.871313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Zuko</td>\n",
       "      <td>8.794207</td>\n",
       "      <td>10.565189</td>\n",
       "      <td>12.344713</td>\n",
       "      <td>7.050011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.359111</td>\n",
       "      <td>7.855921</td>\n",
       "      <td>10.890628</td>\n",
       "      <td>...</td>\n",
       "      <td>33.439299</td>\n",
       "      <td>43.123686</td>\n",
       "      <td>34.130294</td>\n",
       "      <td>9.409731</td>\n",
       "      <td>29.731870</td>\n",
       "      <td>10.266614</td>\n",
       "      <td>26.831976</td>\n",
       "      <td>21.219351</td>\n",
       "      <td>10.319942</td>\n",
       "      <td>31.790223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 character normalized_word_count                        \\\n",
       "book_num                                       1                         \n",
       "chapter_num                                    1          2          3   \n",
       "0                     Aang             22.573198  24.486191  23.818840   \n",
       "1              Actor Sokka              0.000000   0.000000   0.000000   \n",
       "2               Actor Zuko              0.000000   0.000000   0.000000   \n",
       "3             Actress Aang              0.000000   0.000000   0.000000   \n",
       "4            Actress Azula              0.000000   0.000000   0.000000   \n",
       "..                     ...                   ...        ...        ...   \n",
       "118                   Yung              0.000000   0.000000   0.000000   \n",
       "119                    Zei              0.000000   0.000000   0.000000   \n",
       "120           Zhang leader              0.000000   0.000000   0.000000   \n",
       "121                   Zhao              0.000000   0.000000  18.477942   \n",
       "122                   Zuko              8.794207  10.565189  12.344713   \n",
       "\n",
       "                                                                              \\\n",
       "book_num                                                                       \n",
       "chapter_num          4          5         6          7          8          9   \n",
       "0            25.192774  34.644701  7.538701  33.473183  20.917059  17.141792   \n",
       "1             0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "2             0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "3             0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "4             0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "..                 ...        ...       ...        ...        ...        ...   \n",
       "118           0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "119           0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "120           0.000000   0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "121           0.000000   0.000000  0.000000   0.000000   8.871313   0.000000   \n",
       "122           7.050011   0.000000  0.000000   7.359111   7.855921  10.890628   \n",
       "\n",
       "             ...                                                        \\\n",
       "book_num     ...          3                                              \n",
       "chapter_num  ...         12         13         14        15         16   \n",
       "0            ...  13.824212  29.143308   0.000000  0.000000  16.008598   \n",
       "1            ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "2            ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "3            ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "4            ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "..           ...        ...        ...        ...       ...        ...   \n",
       "118          ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "119          ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "120          ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "121          ...   0.000000   0.000000   0.000000  0.000000   0.000000   \n",
       "122          ...  33.439299  43.123686  34.130294  9.409731  29.731870   \n",
       "\n",
       "                                                                    \n",
       "book_num                                                            \n",
       "chapter_num         17         18         19         20         21  \n",
       "0             7.630322  16.210215  26.136514   0.000000  15.506636  \n",
       "1             7.799443   0.000000   0.000000   0.000000   0.000000  \n",
       "2             5.153203   0.000000   0.000000   0.000000   0.000000  \n",
       "3             8.018305   0.000000   0.000000   0.000000   0.000000  \n",
       "4             4.317549   0.000000   0.000000   0.000000   0.000000  \n",
       "..                 ...        ...        ...        ...        ...  \n",
       "118           0.000000   0.000000   0.000000   0.000000   0.000000  \n",
       "119           0.000000   0.000000   0.000000   0.000000   0.000000  \n",
       "120           0.000000   0.000000   0.000000   0.000000   0.000000  \n",
       "121           0.000000   0.000000   0.000000   0.000000   0.000000  \n",
       "122          10.266614  26.831976  21.219351  10.319942  31.790223  \n",
       "\n",
       "[123 rows x 62 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_character_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5add2d35-1282-428e-a034-b2737bedc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# good: 12\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, init='k-means++', \n",
    "                n_init='auto', max_iter=300, verbose=0, random_state=None, copy_x=True, algorithm='lloyd') #tol=0.0001,\n",
    "# kmeans.fit(df_character_word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51cefbb2-756a-4ed1-bb1e-c0d8dd8f6ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/6nv9m_7j7wnb5x_4cztrwm0h0000gp/T/ipykernel_56718/2966549090.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  X = df_character_word_vector.drop(columns=['character']) # group character from majority class (?)\n"
     ]
    }
   ],
   "source": [
    "X = df_character_word_vector.drop(columns=['character']) # group character from majority class (?)\n",
    "pred_y = kmeans.fit_predict(X)\n",
    "\n",
    "# 3. Visualize\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=pred_y, s=50, cmap='viridis')\n",
    "# Plot centroids\n",
    "# plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "#             s=200, c='red', marker='X', label='Centroids')\n",
    "# plt.title(\"KMeans Visualization\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "836f7714-01c5-4767-92a0-fb0d62f6a11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  3,  3,  3,  3,  3,  7,  3,  3, 11,  3,  3,  3,  3,  3,  3,\n",
       "       10,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3, 17, 12,  3,  3,  3,  0,  3,  8,  3,  3, 16,  3,  3,  3,\n",
       "        3,  6,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3, 15,  3,  3,\n",
       "        3,  3,  3,  3,  3, 13, 19,  3,  3,  9,  3,  3,  3,  3,  3,  3,  3,\n",
       "        5,  3,  9,  3,  1,  3,  3, 15,  3,  3,  3,  3,  3, 14,  1,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3, 18,  4], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590a4811-c7df-4f52-8ffa-db276c904f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=5, min_samples=2, algorithm='auto')\n",
    "pred_y = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "499fc23b-0b03-43cf-a12b-a4dadedfbc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  0,  0,  0,  0, -1, -1, -1,  0, -1,  1,  2,  0, -1, -1, -1,\n",
       "       -1,  0,  3,  4,  5,  0,  0,  0, -1, -1,  2,  2,  6, -1, -1,  6, -1,\n",
       "        0, -1, -1, -1, -1,  0,  0,  7, -1, -1, -1,  0, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  0,  0, -1, -1,  7,  0, -1,  6, -1, -1,  0,  0, -1,  1,  0,\n",
       "        8,  5, -1, -1, -1, -1, -1,  8, -1, -1,  0, -1,  6,  5,  0,  0, -1,\n",
       "       -1, -1, -1,  0, -1, -1,  0, -1,  0,  0,  4,  0, -1, -1, -1, -1,  0,\n",
       "        0,  0, -1, -1, -1, -1,  0,  3,  0,  0, -1,  0, -1,  0, -1, -1,  0,\n",
       "       -1,  2, -1, -1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b74221a-9848-4b7f-a66b-46e863aa0a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    Gansu\n",
       "32      Gow\n",
       "60      Lee\n",
       "80     Sela\n",
       "Name: character, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_character_word_vector['character'].iloc[np.where(pred_y == 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa24f83-db61-4044-ab2f-4f0ff121d7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b8ade84-05c6-4e70-ac54-75df6fa4b575",
   "metadata": {},
   "source": [
    "#### 2.1.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30bd5a25-4534-476f-8704-15986894a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_characters = ['Aang', 'Katara', 'Sokka', 'Zuko', 'Toph']\n",
    "secondary_characters = ['Iroh', 'Azula', 'Suki', 'Jet', 'Long Feng'] # between 50 and 100 words spoken\n",
    "# other_characters = # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334cb485-797e-4835-81c9-8a9f1f36d576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4625d3c-dbc6-4303-b711-f5db0147d458",
   "metadata": {},
   "source": [
    "### 3. Prediction - Episode Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250ab16-fb2c-43b2-8b66-4cedc53d5669",
   "metadata": {},
   "source": [
    "### 3.1. Dummy - Mean Prediction by book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75462808-f21a-4872-a18a-28f8d81ce31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_book_rating = duckdb.sql(f\"\"\"\n",
    "select\n",
    "    book_num, AVG(TRY_CAST(imdb_rating AS FLOAT)) as avg_rating\n",
    "from '{AVATAR_URL}'\n",
    "group by book_num\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da997a62-3fc1-4ee5-b0d6-c3e99b0d848d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_num</th>\n",
       "      <th>avg_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>8.862371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.287778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.730077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_num  avg_rating\n",
       "0         3    8.862371\n",
       "1         1    8.287778\n",
       "2         2    8.730077"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_book_rating.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c9b8106-cec3-41cc-a90d-836cf95420d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_book_rating = duckdb.sql(f\"\"\"\n",
    "WITH chapter_avg AS (\n",
    "    SELECT\n",
    "        book_num,\n",
    "        chapter_num,\n",
    "        AVG(TRY_CAST(imdb_rating AS FLOAT)) AS chapter_avg_rating\n",
    "    FROM '{AVATAR_URL}'\n",
    "    WHERE\n",
    "        imdb_rating not null\n",
    "    GROUP BY book_num, chapter_num\n",
    "),\n",
    "book_avg AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        AVG(chapter_avg_rating) OVER (PARTITION BY book_num) AS book_avg_rating\n",
    "    FROM chapter_avg\n",
    "),\n",
    "prediction_rating AS (\n",
    "    SELECT\n",
    "        (chapter_avg_rating - book_avg_rating) * \n",
    "        (chapter_avg_rating - book_avg_rating) AS mse\n",
    "    FROM book_avg\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    SUM(mse) / COUNT(*) AS mse\n",
    "FROM prediction_rating\n",
    "\"\"\")\n",
    "\n",
    "# select * from chapter_avg\n",
    "# order by book_num, chapter_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30c5b66d-6cd0-44c4-b63f-f6cdb5112394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mse\n",
      "0  0.272674\n"
     ]
    }
   ],
   "source": [
    "print(res_book_rating.df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b64dca36-7bb9-4650-b959-b145bde525a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1\n",
      "NA\n"
     ]
    }
   ],
   "source": [
    "print(res_avatar.df()['imdb_rating'].min())\n",
    "print(res_avatar.df()['imdb_rating'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0788bf5-3c3d-48b0-bbf0-9ec1e1e208f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.1.2. Dummy - Mean Prediction by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d13ae-b5e2-4c12-8947-5bf657af6d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981905c-04f4-4207-9ce9-18cb1471de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04dc91-3a83-4659-8fc6-4af834ce3ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac822e-0af7-42ad-8e98-5541408be501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df6fb1-5cb4-44f3-9dbf-95e88f95dfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddf97d-cc8e-4b38-a72d-0ef1f8c6c6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b30a3-bf42-4329-ac36-8bc5e414f8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56959322-8a84-48e5-822c-fe783f84c01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd07f68-f3b5-47ef-9917-4726730da9cf",
   "metadata": {},
   "source": [
    "### 3.2. Based on writer/director (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5457acb9-37a4-4960-bcc2-5f9623a13299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_director = res_avatar.df()[['book_num', 'chapter_num', 'writer', 'director', 'imdb_rating']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdf74c60-2ab5-450d-85cb-0004991cfb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "df_writer_director['writer'] = df_writer_director['writer'].str.split(', ')\n",
    "df_writer_director['director'] = df_writer_director['director'].str.split(', ')\n",
    "\n",
    "# \n",
    "df_writer_director = df_writer_director.loc[df_writer_director['imdb_rating'] != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "003f9e28-1a94-482c-b628-af5a178f3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_writer_director[['writer', 'director']], \n",
    "    df_writer_director['imdb_rating'], \n",
    "    stratify=df_writer_director[['book_num']], \n",
    "    test_size=0.2,\n",
    ")\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_test, y_test, \n",
    "#     test_size=0.2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "352e91b3-2e67-4779-ae43-284f179f03a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emulie/Projects/tidytuesday/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:1007: UserWarning: unknown class(es) ['Andrew Huebner', 'Elizabeth Welch Ehasz (story editor)', 'Gary Scheppke', 'Giancarlo Volpe', 'Joann Estoesta', \"John O'Bryan (story editor)\", 'Justin Ridge', 'Lauren MacMullan', 'Lisa Wahlander', 'Matthew Hubbard', '\\u200eMichael Dante DiMartino'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- encoder\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Writer encoder\n",
    "mlb_writer = MultiLabelBinarizer()\n",
    "writer_train_ohe = mlb_writer.fit_transform(X_train['writer'])\n",
    "writer_test_ohe  = mlb_writer.transform(X_test['writer'])\n",
    "\n",
    "# Director encoder\n",
    "mlb_director = MultiLabelBinarizer()\n",
    "director_train_ohe = mlb_director.fit_transform(X_train['director'])\n",
    "director_test_ohe  = mlb_director.transform(X_test['director'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d5babcb-a413-4b95-9228-9695ecf08554",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train_df = pd.DataFrame(\n",
    "    writer_train_ohe,\n",
    "    columns=[f\"writer_{c}\" for c in mlb_writer.classes_],\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "writer_test_df = pd.DataFrame(\n",
    "    writer_test_ohe,\n",
    "    columns=[f\"writer_{c}\" for c in mlb_writer.classes_],\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "director_train_df = pd.DataFrame(\n",
    "    director_train_ohe,\n",
    "    columns=[f\"director_{c}\" for c in mlb_director.classes_],\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "director_test_df = pd.DataFrame(\n",
    "    director_test_ohe,\n",
    "    columns=[f\"director_{c}\" for c in mlb_director.classes_],\n",
    "    index=X_test.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7296f69-f93f-418b-8176-c2108bb7a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.concat(\n",
    "    [writer_train_df, director_train_df],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_test_final = pd.concat(\n",
    "    [writer_test_df, director_test_df],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cd0ed57-b9ed-4e40-82c2-e0919256a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- MODEL: xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_final, y_train)\n",
    "pred = model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5871bf1-3fbb-4c36-92f9-2a72487cfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "458f537b-d513-49cd-a644-4d1585be987b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8373823068963092"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98648031-f9fd-4b98-9e78-cf898bed5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_final['rating'] = y_test\n",
    "# X_test_final['rating_pred'] = pred\n",
    "# X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ac4093e-e5ee-4801-a170-8a56b084ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- MODEL: Lasso Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "lasso = LassoLars(alpha=.1)\n",
    "lasso.fit(X_train_final, y_train)\n",
    "pred = lasso.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9aac1327-c23f-4876-8d8f-7e990e681ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa754778-bc8d-4a3b-a753-c12b3c417937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.618665819189434"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21bfa9e1-920f-4897-b0f5-6b4bc7f53b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6667184544128724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_final, y_train)\n",
    "pred = lr.predict(X_test_final)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c09d4-5343-430b-9b43-65aad46a4811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d581c7f-cc52-4f85-bd83-0f2f2ec0c20e",
   "metadata": {},
   "source": [
    "### 3.2. Based on writer/director (Target, Frequency Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ed39bd5-fdcb-4296-be04-4c805273f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_writer_director = res_avatar.df()[['book_num', 'chapter_num', 'writer', 'director', 'imdb_rating']].drop_duplicates()\n",
    "\n",
    "# data cleaning\n",
    "df_writer_director['writer'] = df_writer_director['writer'].str.split(', ')\n",
    "# df_writer_director['director'] = df_writer_director['director'].str.split(', ')\n",
    "\n",
    "# \n",
    "df_writer_director = df_writer_director.loc[df_writer_director['imdb_rating'] != 'NA']\n",
    "df_writer_director['director'] = df_writer_director['director'].astype(str)\n",
    "df_writer_director['imdb_rating'] = df_writer_director['imdb_rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7391daa-f501-4da2-beff-5cde55171c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_writer_director[['book_num', 'chapter_num', 'writer', 'director']]\n",
    "y = df_writer_director[['imdb_rating']]\n",
    "stratify_col = df_writer_director['book_num']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "308e4045-c8ed-458d-8231-573d4bbd8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: 0.5585597101256908\n",
      "Fold 1: 0.3837047610503748\n",
      "Fold 2: 0.6996145353654102\n",
      "Fold 3: 0.5054715411479128\n",
      "Fold 4: 0.6551363214563752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.preprocessing import TargetEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, stratify_col)):\n",
    "    # --- split dataset\n",
    "    X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "    y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "    # --- encoding\n",
    "    NUM_WRITERS = 2\n",
    "\n",
    "    # encoder_director\n",
    "    encoder_director = TargetEncoder()\n",
    "    X_train['director_encoded'] = encoder_director.fit_transform(X_train[['director']], y_train)\n",
    "    X_test['director_encoded'] = encoder_director.transform(X_test[['director']])\n",
    "\n",
    "    # encoder_writer\n",
    "    writer_counts = X_train['writer'].explode().value_counts() #.reset_index()\n",
    "    writer_freq = writer_counts.to_dict()\n",
    "    X_train['writer_freq'] = X_train['writer'].apply(\n",
    "        lambda writers: [writer_freq[w] if w in writer_freq else 0 for w in writers]\n",
    "    )\n",
    "    X_train['writer_freq'] = X_train['writer_freq'].apply(lambda x: sorted(x, reverse=True))\n",
    "    X_train['writer_freq'] = X_train['writer_freq'].apply(lambda x: x[:NUM_WRITERS] if len(x) >= NUM_WRITERS else x + [0] * (NUM_WRITERS-len(x)))\n",
    "\n",
    "    X_test['writer_freq'] = X_test['writer'].apply(\n",
    "        lambda writers: [writer_freq[w] if w in writer_freq else 0 for w in writers]\n",
    "    )\n",
    "    X_test['writer_freq'] = X_test['writer_freq'].apply(lambda x: sorted(x, reverse=True))\n",
    "    X_test['writer_freq'] = X_test['writer_freq'].apply(lambda x: x[:NUM_WRITERS] if len(x) >= NUM_WRITERS else x + [0] * (NUM_WRITERS-len(x)))\n",
    "\n",
    "    writer_cols = [f\"writer_freq_{i}\" for i in range(NUM_WRITERS) ]\n",
    "    X_train[writer_cols] = pd.DataFrame(X_train['writer_freq'].to_list(), index=X_train.index)\n",
    "    X_test[writer_cols] = pd.DataFrame(X_test['writer_freq'].to_list(), index=X_test.index)\n",
    "\n",
    "    # --- feature engineered datasets\n",
    "    X_train_final = X_train[['book_num']]\n",
    "    X_test_final = X_test[['book_num']]\n",
    "    \n",
    "    # X_train_final = X_train[['director_encoded']]\n",
    "    # X_test_final = X_test[['director_encoded']]\n",
    "    \n",
    "    # X_train_final = X_train[['director_encoded'] + writer_cols]\n",
    "    # X_test_final = X_test[['director_encoded'] + writer_cols]\n",
    "    \n",
    "    # X_train_final = X_train[['director_encoded', 'book_num'] + writer_cols]\n",
    "    # X_test_final = X_test[['director_encoded', 'book_num'] + writer_cols]\n",
    "\n",
    "    # --- model\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=5, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_train_final, y_train)\n",
    "    pred = model.predict(X_test_final)\n",
    "\n",
    "    # X_test['pred'] = pred\n",
    "    # X_test['imdb_rating'] = y_test\n",
    "    # print(X_test[['imdb_rating', 'pred']])\n",
    "\n",
    "    rmse = rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "    print(f\"Fold {i}: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e73c7-3cad-4a2d-b9c7-1652b9648032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea385b89-31f9-4887-b5e6-9a5958474c58",
   "metadata": {},
   "source": [
    "#### Cleaner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "423ba613-4f67-44eb-b2d4-1419f856a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from category_encoders import TargetEncoder\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# Custom transformer for writer frequency encoding\n",
    "class WriterFrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_writers=2):\n",
    "        self.num_writers = num_writers\n",
    "        self.writer_freq = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate writer frequencies from training data\n",
    "        writer_counts = X['writer'].explode().value_counts()\n",
    "        self.writer_freq = writer_counts.to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Apply frequency encoding\n",
    "        X['writer_freq'] = X['writer'].apply(\n",
    "            lambda writers: [self.writer_freq.get(w, 0) for w in writers]\n",
    "        )\n",
    "        X['writer_freq'] = X['writer_freq'].apply(lambda x: sorted(x, reverse=True))\n",
    "        X['writer_freq'] = X['writer_freq'].apply(\n",
    "            lambda x: x[:self.num_writers] if len(x) >= self.num_writers \n",
    "            else x + [0] * (self.num_writers - len(x))\n",
    "        )\n",
    "        \n",
    "        # Create separate columns for each writer frequency\n",
    "        writer_cols = [f\"writer_freq_{i}\" for i in range(self.num_writers)]\n",
    "        X[writer_cols] = pd.DataFrame(X['writer_freq'].to_list(), index=X.index)\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Custom transformer for feature selection\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_set='book_only'):\n",
    "        \"\"\"\n",
    "        feature_set options:\n",
    "        - 'book_only': ['book_num']\n",
    "        - 'director_only': ['director_encoded']\n",
    "        - 'director_writer': ['director_encoded'] + writer_cols\n",
    "        - 'all': ['director_encoded', 'book_num'] + writer_cols\n",
    "        \"\"\"\n",
    "        self.feature_set = feature_set\n",
    "        self.num_writers = 2\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        writer_cols = [f\"writer_freq_{i}\" for i in range(self.num_writers)]\n",
    "        \n",
    "        if self.feature_set == 'book_only':\n",
    "            return X[['book_num']]\n",
    "        elif self.feature_set == 'director_only':\n",
    "            return X[['director_encoded']]\n",
    "        elif self.feature_set == 'director_writer':\n",
    "            return X[['director_encoded'] + writer_cols]\n",
    "        elif self.feature_set == 'all':\n",
    "            return X[['director_encoded', 'book_num'] + writer_cols]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown feature_set: {self.feature_set}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4defc865-35aa-44b4-9b02-e0ef1253cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "experiment_name = \"avatar_imdb_prediction\"\n",
    "\n",
    "# experiment_id = mlflow.create_experiment(experiment_name)\n",
    "\n",
    "try:\n",
    "    experiment_id = client.create_experiment(\n",
    "        experiment_name,\n",
    "        artifact_location=Path.cwd().joinpath(\"mlruns\").as_uri(),\n",
    "        tags={\"purpose\": \"example\", \"dataset\": \"iris\"}\n",
    "    )\n",
    "except Exception as e:\n",
    "    # If the experiment already exists, get its ID\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    experiment_id = experiment.experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f2e3d56-c39e-4c32-b825-b6f155ac23a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run fold_0 at: https://mlflow-cloud-service-400512913085.us-central1.run.app/#/experiments/14/runs/14a973b713914ef38386ea99fd076c22\n",
      "🧪 View experiment at: https://mlflow-cloud-service-400512913085.us-central1.run.app/#/experiments/14\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([ 0,  2,  3,  4,  6,  7,  8,  9, 10, 11, 13, 14, 15, 17, 18, 19, 20, 21,\\n       22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43,\\n       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57],\\n      dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n\u001b[32m     25\u001b[39m X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m y_train, y_test = \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m, y[test_index]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Director encoding\u001b[39;00m\n\u001b[32m     29\u001b[39m encoder_director = TargetEncoder()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tidytuesday/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tidytuesday/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/tidytuesday/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index([ 0,  2,  3,  4,  6,  7,  8,  9, 10, 11, 13, 14, 15, 17, 18, 19, 20, 21,\\n       22, 23, 24, 25, 26, 28, 29, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 43,\\n       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57],\\n      dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "NUM_WRITERS = 2\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=4)\n",
    "feature_set = 'book_only'\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': \"reg:squarederror\",\n",
    "    'n_estimators': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "all_rmse = []\n",
    "all_mae = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, stratify_col)):\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name=f\"fold_{i}\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"fold\", i)\n",
    "        mlflow.log_param(\"feature_set\", feature_set)\n",
    "        mlflow.log_param(\"num_writers\", NUM_WRITERS)\n",
    "        mlflow.log_param(\"n_splits\", 5)\n",
    "        mlflow.log_params(xgb_params)\n",
    "        \n",
    "        # Split dataset\n",
    "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Director encoding\n",
    "        encoder_director = TargetEncoder()\n",
    "        X_train['director_encoded'] = encoder_director.fit_transform(X_train[['director']], y_train)\n",
    "        X_test['director_encoded'] = encoder_director.transform(X_test[['director']])\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('writer_encoder', WriterFrequencyEncoder(num_writers=NUM_WRITERS)),\n",
    "            ('feature_selector', FeatureSelector(feature_set=feature_set)),\n",
    "            ('model', xgb.XGBRegressor(**xgb_params))\n",
    "        ])\n",
    "        \n",
    "        # Fit and predict\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "        mae = np.mean(np.abs(y_test - pred))\n",
    "        \n",
    "        all_rmse.append(rmse)\n",
    "        all_mae.append(mae)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"train_size\", len(y_train))\n",
    "        mlflow.log_metric(\"test_size\", len(y_test))\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(pipeline, f\"model_fold_{i}\")\n",
    "        \n",
    "        # Add tags\n",
    "        mlflow.set_tag(\"model_type\", \"xgboost\")\n",
    "        mlflow.set_tag(\"pipeline\", \"sklearn\")\n",
    "        \n",
    "        print(f\"Fold {i}: RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== Cross-Validation Summary ===\")\n",
    "print(f\"Mean RMSE: {np.mean(all_rmse):.4f} (+/- {np.std(all_rmse):.4f})\")\n",
    "print(f\"Mean MAE: {np.mean(all_mae):.4f} (+/- {np.std(all_mae):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c3c52b-79a2-43b1-a362-ddf228465618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218b3d9-52bd-4195-aec1-b39aafa77611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012061d-3ec6-4f3a-9d48-3b3274747cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40832148-a46f-4240-a82c-bb0f768becac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7122b81-8daf-40d7-9ac8-36b3d1a70138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0f0421f-c3e3-4d17-9640-24c0380a0cbd",
   "metadata": {},
   "source": [
    "### 3.3. Based on character word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c206a-34c7-4ffa-88dc-4c79c2454c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: main_characters, 2: secondary_characters; 3: other; 4: book_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11244ac-ffdf-40b9-952b-582447956b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12e9ca-3133-42ec-89b7-bef874343ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc3284-8635-4e2f-bffc-846c7ab12bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15362259-774c-4db3-82fc-9c712e73e818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
