{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d65a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yukikongju/Projects/tidytuesday/financials_news_sentimentanalysis\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e9b9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de47b8b",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7518b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df = df[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17cd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {'positive': 2, 'negative': 0, 'neutral': 1}\n",
    "y = df['Sentiment'].apply(lambda x: sentiment_dict.get(str(x))).tolist()\n",
    "X = df['Sentence'].tolist() # FIXME: should we split first, then vectorize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e1b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6601552",
   "metadata": {},
   "source": [
    "### Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f648ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Normalizer, self).__init__()\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, sentences, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, sentences, labels=None):\n",
    "        return [self.normalize(sentence) for sentence in sentences]\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        pass\n",
    "\n",
    "class LemmerNormalizer(Normalizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LemmerNormalizer, self).__init__()\n",
    "        \n",
    "    def normalize(self, sentence):\n",
    "        words = []\n",
    "        for word in sentence.split(' '):\n",
    "            words.append(self.lemmatizer.lemmatize(word))\n",
    "        return ' '.join(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7d4c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NGramsVectorizer(BaseEstimator, TransformerMixin):  \n",
    "class NGramsVectorizer(BaseEstimator):    \n",
    "    \n",
    "    \n",
    "    def __init__(self, n_grams=2):\n",
    "        super(NGramsVectorizer, self).__init__()\n",
    "        self.n_grams = n_grams\n",
    "        self.vocab = None\n",
    "        self.word_dict = None\n",
    "    \n",
    "    def fit(self, sentences, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, sentences, labels):\n",
    "        # build vocab\n",
    "        self._build_vocab(sentences)\n",
    "        \n",
    "        # build word dict\n",
    "        self._build_word_dict()\n",
    "        \n",
    "        # get ngram \n",
    "        ngrams = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            sentence_grams = self._get_ngrams_from_sentence(sentence)\n",
    "            for gram in sentence_grams:\n",
    "                gram_idx = [self.word_dict.get(word) for word in gram]\n",
    "#                 ngrams.append((gram, label))\n",
    "                ngrams.append((gram_idx, label))\n",
    "                \n",
    "        return ngrams\n",
    "    \n",
    "    def _get_ngrams_from_sentence(self, sentence):\n",
    "        \"\"\" \n",
    "        >>> sentence = \"Report suggests that company should go bankrupt by the end of the quarter\"\n",
    "        >>> [['Report', 'suggests'], ['suggests', 'that'], ..,  ['the', 'quarter'] ]\n",
    "        \"\"\"\n",
    "        words = sentence.split(' ')\n",
    "        ngrams = [words[i:i+n] for i in range(0, len(words)-1)]\n",
    "        return ngrams\n",
    "        \n",
    "    \n",
    "    def _build_vocab(self, sentences):\n",
    "        self.vocab = set([word for sentence in sentences for word in sentence.split(' ')])\n",
    "        \n",
    "    \n",
    "    def _build_word_dict(self):\n",
    "        self.word_dict = { word: i for i, word in enumerate(self.vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3383c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, n = \"Report suggests that company should go bankrupt by the end of the quarter\", 2\n",
    "text = text.split(' ')\n",
    "ngrams = [text[i:i+n] for i in range(0, len(text)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5e4a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = NGramsVectorizer()\n",
    "ngrams_vect = vectorizer.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a459adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67556f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54747c02",
   "metadata": {},
   "source": [
    "### Split data into Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9140533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_x, ngrams_y = [], []\n",
    "for ngram_x, ngram_y in ngrams_vect:\n",
    "    ngrams_x.append(ngram_x)\n",
    "    ngrams_y.append(ngram_y)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ngrams_x, ngrams_y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824566a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffbcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c992887",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40f81f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5266dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.15      0.04        48\n",
      "           1       0.88      0.61      0.72      2051\n",
      "           2       0.14      0.33      0.19       265\n",
      "\n",
      "    accuracy                           0.57      2364\n",
      "   macro avg       0.35      0.36      0.32      2364\n",
      "weighted avg       0.78      0.57      0.65      2364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(XGBClassifier(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5639d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.60      0.75      2364\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60      2364\n",
      "   macro avg       0.33      0.20      0.25      2364\n",
      "weighted avg       1.00      0.60      0.75      2364\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yukikongju/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yukikongju/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yukikongju/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(GaussianNB(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "82942ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.14      0.15       367\n",
      "           1       0.60      0.62      0.61      1374\n",
      "           2       0.28      0.29      0.28       623\n",
      "\n",
      "    accuracy                           0.46      2364\n",
      "   macro avg       0.35      0.35      0.35      2364\n",
      "weighted avg       0.45      0.46      0.45      2364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(DecisionTreeClassifier(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4526a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
