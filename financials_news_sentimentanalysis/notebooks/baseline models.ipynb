{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe577e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yukikongju/Projects/tidytuesday/financials_news_sentimentanalysis\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b9d8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5318727",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d50ee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "# df = df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3f5b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b952253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c17e9cd7",
   "metadata": {},
   "source": [
    "### Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ee87dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Normalizer, self).__init__()\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, sentences, labels=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        return [self.normalize(sentence) for sentence in sentences]\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        pass\n",
    "\n",
    "\n",
    "class StopWordLemmerNormalizer(Normalizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StopWordLemmerNormalizer, self).__init__()\n",
    "        \n",
    "    def normalize(self, sentence):\n",
    "        words = []\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in self.stop_words: \n",
    "                words.append(self.lemmatizer.lemmatize(word))\n",
    "        return ' '.join(words)\n",
    "    \n",
    "class LemmerNormalizer(Normalizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LemmerNormalizer, self).__init__()\n",
    "        \n",
    "    def normalize(self, sentence):\n",
    "        words = []\n",
    "        for word in sentence.split(' '):\n",
    "            words.append(self.lemmatizer.lemmatize(word))\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d71135a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OneHotVectorizer, self).__init__()\n",
    "        self.vectorizer = CountVectorizer(binary=True)\n",
    "    \n",
    "    def fit(self, sentences, labels = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        freqs = self.vectorizer.fit_transform(sentences)\n",
    "        return freqs.toarray()\n",
    "\n",
    "\n",
    "class TFIDFVectorizer(BaseEstimator, TransformerMixin):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramsVectorizer(BaseEstimator, TransformerMixin): # TODO\n",
    "    \n",
    "    def __init__(self, n_grams=2):\n",
    "        super(NGramsVectorizer, self).__init__()\n",
    "        self.n_grams = n_grams\n",
    "    \n",
    "    def fit(self, sentences, labels=None):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VectVectorizer(BaseEstimator, TransformerMixin): # TODO\n",
    "    \n",
    "    def __init__(self, n_dim=1000):\n",
    "        super(Word2VectVectorizer, self).__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, sentences, labels=None):\n",
    "        self.model = word2vec.Word2Vec(corpus, vector_size=self.n_dim, window=5, min_count=1)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ec337ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 1 0 1 1]\n",
      " [0 1 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('normalizer', StopWordLemmerNormalizer()),\n",
    "    ('vectorizer', OneHotVectorizer())\n",
    "])\n",
    "\n",
    "texts = [\n",
    "    \"The S&P500 is up 50 points today\",\n",
    "    \"Firm to be buyout in the next quarter\"\n",
    "]\n",
    "print(pipeline.fit_transform(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab02f56",
   "metadata": {},
   "source": [
    "### Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b15a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = {'positive': 2, 'negative': 0, 'neutral': 1}\n",
    "y = df['Sentiment'].apply(lambda x: sentiment_dict.get(str(x))).tolist()\n",
    "X = pipeline.fit_transform(df['Sentence'].tolist()) # FIXME: should I split first, then vectorize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "159bb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d95147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29e09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92e712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f70aea",
   "metadata": {},
   "source": [
    "### Train Models with TextNorm + OneHotVect pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9790b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d78941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.34      0.22       119\n",
      "           1       0.87      0.68      0.77      1224\n",
      "           2       0.58      0.77      0.66       410\n",
      "\n",
      "    accuracy                           0.68      1753\n",
      "   macro avg       0.54      0.60      0.55      1753\n",
      "weighted avg       0.76      0.68      0.71      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(XGBClassifier(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9c0cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.22      0.27       423\n",
      "           1       0.54      0.70      0.61       752\n",
      "           2       0.51      0.49      0.50       578\n",
      "\n",
      "    accuracy                           0.51      1753\n",
      "   macro avg       0.48      0.47      0.46      1753\n",
      "weighted avg       0.49      0.51      0.49      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(GaussianNB(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1418d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.17      0.18       272\n",
      "           1       0.69      0.68      0.69       976\n",
      "           2       0.60      0.66      0.63       505\n",
      "\n",
      "    accuracy                           0.59      1753\n",
      "   macro avg       0.50      0.50      0.50      1753\n",
      "weighted avg       0.59      0.59      0.59      1753\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_model_performance(DecisionTreeClassifier(), x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df998ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
